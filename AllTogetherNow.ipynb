{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the data, from two subreddits ##\n",
    "\n",
    "1. Function takes two subreddits, scrapes the top thousand posts from each of them, saves it to a csv. Combines the DFs, maps out the target column. Pre-processes the title data appropriately. returns the clean df (X) and target(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, time, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, Sno\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets a subreddit name and scrapes the top thousand entries in JSON, returning a list of them\n",
    "def scrape_subr(subr):\n",
    "    url= \"https://www.reddit.com/r/\"+subr+'/top.json?t=all'; \n",
    "    posts=[];\n",
    "    after= None; \n",
    "    \n",
    "    for _ in range(40):\n",
    "            if after == None:\n",
    "                current_url = url\n",
    "            else:\n",
    "                current_url = url + '&after=' + after\n",
    "            print(f'/r/{subr} Page {_}:', current_url);\n",
    "\n",
    "            #make the request and handle status code, add 2 sec sleep\n",
    "\n",
    "            res= requests.get(current_url, headers={'User-agent': 'DataSci 5.5'})\n",
    "\n",
    "            if res.status_code != 200:\n",
    "                print('Status error', res.status_code)\n",
    "                break\n",
    "\n",
    "            current_dict = res.json()\n",
    "            current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "            posts.extend(current_posts)\n",
    "\n",
    "            after= current_dict['data']['after']\n",
    "\n",
    "            time.sleep(2)\n",
    "    df = pd.DataFrame(posts).drop('Unnamed: 0', 1)\n",
    "    df.to_csv(f'./data_csvs/{subr}_raw.csv', index=False)\n",
    "    # For seperation of concerns, could pull out a \"clean_df/csv\" function\n",
    "    df_clean = df[['title', 'subreddit']]\n",
    "    df_clean.to_csv(f'./data_csvs/{subr}_clean.csv', index=False)\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redditize(subr1, subr2):\n",
    "    df_list=[]\n",
    "    for subr in [subr1, subr2]:\n",
    "        #scrape the subreddit, save it as a csv, add it to our list\n",
    "        df_list.append(scrape_subr(subr));\n",
    "    #concatenate the dfs into one\n",
    "    df= pd.concat(df_list, ignore_index=True)\n",
    "    #clean up the title column: remove punctuations and lowercase it\n",
    "    #### NOTE: If other  transformations are required, apply the following (or more) ####\n",
    "#     stopWords = set(stopwords.words('english'))\n",
    "#     df['qmark']= df['title'].apply(lambda x: 1 if '?' in x else 0 )\n",
    "#     df['words_not_stopword'] = df['processed'].apply(lambda x: len([t for t in x.split(' ') if t not in stopWords]))\n",
    "    df['processed'] = df['title'].apply(lambda x: re.sub(r'[^\\w\\s]','', x.lower()))\n",
    "    # This will be our X and y:\n",
    "    map_dict= {\n",
    "        subr1: 1,\n",
    "        subr2: 0\n",
    "    }\n",
    "    df['subreddit'] = df['subreddit'].map(map_dict)\n",
    "    return df['processed'], df['subreddit']\n",
    "#   right way to call is >> X, y = redditize('subr1', 'subr2')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['processed'], df['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Train test split and fit our model to the train, score on split #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer needed for our model (may pickle for later)\n",
    "from nltk import word_tokenize          \n",
    "\n",
    "class SnowballTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.sbs = SnowballStemmer('english')\n",
    "    def __call__(self, doc):\n",
    "        return [self.sbs.stem(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our pickled model\n",
    "model = joblib.load('./tf_svc_pipe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=9500, min_df=1,\n",
       "        ngram_range=(1, 4), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9966577540106952"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9619238476953907"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)\n",
    "#This is not horrible, but not great (the original two subr got a 98%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes a df, splits, it and processes the accuracy (and even confusion matrix!) on the two. \n",
    "def run_model(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"Training Accuracy: {model.score(X_train, y_train)}\")\n",
    "    print(f\"Testing Accuracy: {model.score(X_test, y_test)}\")\n",
    "    tfidf = model.named_steps.tfidf\n",
    "    svc = model.named_steps.svc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = model.named_steps.tfidf\n",
    "svc = model.named_steps.svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_coefs= pd.DataFrame(svc.coef_.toarray()[0], tfidf.get_feature_names()).sort_values(0)[-5:]\n",
    "least_coefs= pd.DataFrame(svc.coef_.toarray()[0], tfidf.get_feature_names()).sort_values(0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>1.529895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pipelin</th>\n",
       "      <td>1.693543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rig</th>\n",
       "      <td>2.175624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opec</th>\n",
       "      <td>2.382832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oil</th>\n",
       "      <td>6.151136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "us       1.529895\n",
       "pipelin  1.693543\n",
       "rig      2.175624\n",
       "opec     2.382832\n",
       "oil      6.151136"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_coefs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
